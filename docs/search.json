[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gilmore rethinks",
    "section": "",
    "text": "I ran across McElreath’s book (2020) and YouTube video series (Statistical rethinking 2023, 2023) by accident.\nBut Pasteur said that “fortune favors the prepared mind” (“Louis pasteur,” 2004). So, I acknowledge that I have enjoyed these materials in large part because I was looking for something like them. Specifically, I’ve recommended simulation and visualization as vital data analytic techniques without really appreciating how these ideas mesh with Bayesian approaches. I find myself generating directed acyclic graphs (DAGs) without appreciating their utility in data analyses. And, I have read, but only partially grasped the ideas in Judea Pearl’s work on causal modeling (Mackenzie & Pearl, 2020; Pearl, 2009).\nThis site represents my notes and comments on the textbook and the video series as I work my way through them. By creating code, equations, and visualizations, I hope to master the material and add some new Quarto tricks to my toolkit.",
    "crumbs": [
      "Gilmore rethinks"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Gilmore rethinks",
    "section": "",
    "text": "I ran across McElreath’s book (2020) and YouTube video series (Statistical rethinking 2023, 2023) by accident.\nBut Pasteur said that “fortune favors the prepared mind” (“Louis pasteur,” 2004). So, I acknowledge that I have enjoyed these materials in large part because I was looking for something like them. Specifically, I’ve recommended simulation and visualization as vital data analytic techniques without really appreciating how these ideas mesh with Bayesian approaches. I find myself generating directed acyclic graphs (DAGs) without appreciating their utility in data analyses. And, I have read, but only partially grasped the ideas in Judea Pearl’s work on causal modeling (Mackenzie & Pearl, 2020; Pearl, 2009).\nThis site represents my notes and comments on the textbook and the video series as I work my way through them. By creating code, equations, and visualizations, I hope to master the material and add some new Quarto tricks to my toolkit.",
    "crumbs": [
      "Gilmore rethinks"
    ]
  },
  {
    "objectID": "ch-01-03.html",
    "href": "ch-01-03.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "Chapters 1-3"
    ]
  },
  {
    "objectID": "ch-01-03.html#about",
    "href": "ch-01-03.html#about",
    "title": "",
    "section": "About",
    "text": "About\nThis page summarizes work related to chapters 1-3 of Statistical Rethinking (2020).",
    "crumbs": [
      "Chapters 1-3"
    ]
  },
  {
    "objectID": "ch-01-03.html#setup",
    "href": "ch-01-03.html#setup",
    "title": "",
    "section": "Setup",
    "text": "Setup\nSource libraries for more efficient and readable code.\n\n\nCode\nsuppressPackageStartupMessages(library(ggplot2))",
    "crumbs": [
      "Chapters 1-3"
    ]
  },
  {
    "objectID": "ch-01-03.html#components-of-the-model",
    "href": "ch-01-03.html#components-of-the-model",
    "title": "",
    "section": "Components of the model",
    "text": "Components of the model\nSee (2020, pp. 32–35).\n\\[ Pr(W,L|p)=\\frac{(W+L)!}{W!L!}p^W(1-p)^L  \\tag{1}\\]\nThe probability of \\(W\\) and \\(L\\), conditional on \\(p\\) is defined by the formula in Equation 1.\nThe dbinom() function computes this binomial probability. To compute the probability of observing \\(x=6\\) \\(W\\)s in 9 globe tosses with \\(p=4\\), we proceed as indicated below:\n\n```{r}\n#| code-fold: false\ndbinom(x = 6, size = 9, prob = 0.5, log = FALSE)\n```\n\n[1] 0.1640625\n\n\nSo, to compute the binomial probabilities for observing all \\(n+1\\) possible cases, we proceed as follows:\n\n```{r}\n#| code-fold: false\n\n# Generate the data\nn_W &lt;- 0:9\np_n &lt;- purrr::map(n_W, dbinom, size = 9, prob = 0.5, log = FALSE) |&gt;\n  purrr::list_c()\n\nlike_1 &lt;- data.frame(n_obs = n_W, p_n = p_n)\n\n# Plot the data\nlike_1 |&gt;\n  ggplot() +\n  geom_point(aes(x = n_obs, y = p_n)) +\n  theme_classic()\n```\n\n\n\n\n\n\n\n\nTo produce a set of binomial plots based on increasing numbers of samples, we proceed as follows.\n\n```{r}\n#| code-fold: false\n\n# Generate the data\nn_samples &lt;- c(1, 2, 4, 8, 16, 32, 64)\n\nhow_many_W &lt;- function(n) 0:n\nn_W &lt;- purrr::map(n_samples, how_many_W) |&gt;\n  purrr::list_c()\n\nn_states &lt;- rep(n_samples, n_samples+1)\n\np_n &lt;- purrr::map2(n_W, n_states, dbinom, prob = 0.5, log = FALSE) |&gt;\n  purrr::list_c()\n\nlike_2 &lt;- data.frame(n_states = n_states, n_W = n_W, p_n = p_n)\n```\n\n\n\nCode\nlike_2 |&gt;\n  ggplot() +\n  geom_point(aes(x = n_W, y = p_n)) +\n  facet_wrap(n_states, ncol = 3) +\n  theme_classic() +\n  ggtitle(\"Probability density as number of samples increases.\")\n\n\n\n\n\n\n\n\n\nSo, this figure isn’t quite what I want. But it’s a start. What I think I want is a graph that plots the proportion of times \\(W\\) was picked out of \\(n\\) samples vs. the probability density.\n\n```{r}\n#| code-fold: false\n\n# Generate the data\nn_samples &lt;- c(1, 2, 4, 8, 16, 32, 64)\n\nhow_many_W &lt;- function(n) 0:n\nn_W &lt;- purrr::map(n_samples, how_many_W) |&gt;\n  purrr::list_c()\n\nn_states &lt;- rep(n_samples, n_samples+1)\n\np_W &lt;- n_W/n_states\n\np_n &lt;- purrr::map2(n_W, n_states, dbinom, prob = 0.5, log = FALSE) |&gt;\n  purrr::list_c()\n\nlike_3 &lt;- data.frame(n_states = n_states, n_W = n_W, p_W = p_W, p_n = p_n)\n```\n\n\n\nCode\nlike_3 |&gt;\n  ggplot() +\n  geom_point(aes(x = p_W, y = p_n)) +\n  facet_wrap(n_states, ncol = 3) +\n  theme_classic() +\n  ggtitle(\"Probability density for a given proportion of W's as number of samples increases.\")\n\n\n\n\n\n\n\n\n\nYes. That’s it.",
    "crumbs": [
      "Chapters 1-3"
    ]
  }
]